Try on well-known nonlinear datasets (ring, S, etc.)
for ideas, see some of your Evernote saved articles

Compare to preprocess (dimensionality reduction & feature extraction) + base classifiers
Think again about why PCA doesn't work on Basic Model
Basically, PCA-then-LDA won't work because PCA's definition of "useless dimension" is "dimension with low variance" -- however, Basic Model has the same variance in every dimension.
BUT -- PCA will do something if you feed it the joint (X, class) because it will detect covariance with class. (there seem to be better ways of doing this however)

But don't forget that your job description is to work on axis-aligned RPE

The main gist of the theoretical part of the paper was that performance was independent of (? linear/sublienar in?) d.
But is that also true for DR/FE algorithms?

Compare ensemble-of-projections with autoencoders

Diffusion maps -- review this

The base classifier sort of becomes like a kernel in some sense? review kernel trick/kernel PCA

PARTIAL LEAST SQUARES

Model 1 is really similar to ring dataset (in that decision boundary is circlular/ovular)
You should try some of the nonlinear DR methods that work well for that (locally linear embeddings)
Or even rbf-svm, I seem to recall that it worked for this problem.





You should add RPE to caret (for it to gain more widespread use)